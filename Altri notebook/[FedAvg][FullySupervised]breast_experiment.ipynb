{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf647e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "#################################\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 16, 300\n",
    "IMAGE_SIZE = (600, 600)\n",
    "CROP_SIZE = (224, 224)\n",
    "#########################################\n",
    "DIR = 'dataset/breast'\n",
    "CLIENTS = ['miccai', 'bns']\n",
    "CLIENTS_2 = [cl +'_2' for cl in CLIENTS]\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "#####################################\n",
    "# add the classification segment ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cc0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "LR, WD = 1e-3, 1e-4\n",
    "\n",
    "LAM, BETA, TH = 10, 1.5,0.9\n",
    "VERSION = 1\n",
    "CLIENTS_SUPERVISION = ['labeled', 'labeled']\n",
    "WEIGHTS_CL = [0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560533f",
   "metadata": {},
   "source": [
    "# load training-test path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b6a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data\n",
      "Full training data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Definisci il percorso delle cartelle delle immagini e delle etichette\n",
    "IMAGES_DIR = \"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/imagesTrGray\"\n",
    "LABELS_DIR = \"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/labelsTrGray\"\n",
    "\n",
    "breast_dataset = dict()\n",
    "idx_ = 0\n",
    "denom_ = 0\n",
    "\n",
    "for client, sup in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "    # Carica le immagini e le etichette dal percorso specificato\n",
    "    x_train = glob.glob(os.path.join(IMAGES_DIR, \"*.png\"))\n",
    "    y_train = glob.glob(os.path.join(LABELS_DIR, \"*.png\"))\n",
    "\n",
    "    # Se il cliente non ha supervisione, aggiungi anche le immagini normali\n",
    "    if sup == 'unlabeled':\n",
    "        DATA_TYPE = ['original', 'GT']\n",
    "        for data in DATA_TYPE:\n",
    "            # Carica le immagini normali\n",
    "            normal_images = glob.glob(os.path.join(IMAGES_DIR, \"miccai\", \"classification\", data, \"normal*.jpg\"))\n",
    "            if data == 'GT':\n",
    "                y_train += normal_images\n",
    "            else:\n",
    "                x_train += normal_images\n",
    "\n",
    "    # Scegli casualmente il 10% delle immagini e delle etichette come set di test\n",
    "    num_test_samples = int(0.1 * len(x_train))\n",
    "    test_indices = random.sample(range(len(x_train)), num_test_samples)\n",
    "    x_test = [x_train[i] for i in test_indices]\n",
    "    y_test = [y_train[i] for i in test_indices]\n",
    "    x_train = [x_train[i] for i in range(len(x_train)) if i not in test_indices]\n",
    "    y_train = [y_train[i] for i in range(len(y_train)) if i not in test_indices]\n",
    "\n",
    "    print('Full training data')\n",
    "    WEIGHTS_CL[idx_] = len(x_train)\n",
    "    denom_ += len(x_train)\n",
    "    idx_ += 1\n",
    "    \n",
    "    # Carica il dataset utilizzando la classe Cancer (assicurati di avere la definizione di questa classe)\n",
    "    breast_dataset[client+'_train'] = Cancer(x_train, y_train, train=True,\\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE, CROP_SIZE=CROP_SIZE)\n",
    "\n",
    "    # Carica il dataset di test\n",
    "    breast_dataset[client+'_test'] = Cancer(x_test, y_test, train=False,\\\n",
    "                                            IMAGE_SIZE=IMAGE_SIZE, CROP_SIZE=CROP_SIZE)\n",
    "\n",
    "# Normalizza i pesi dei clienti\n",
    "for idx_ in range(len(WEIGHTS_CL)):\n",
    "    WEIGHTS_CL[idx_] = WEIGHTS_CL[idx_]/denom_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967240b",
   "metadata": {},
   "source": [
    "# storage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24701bd5-51ad-4391-9959-69f1dd17aad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(breast_dataset[client+'_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa76ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_clients, testing_clients = dict(), dict()\n",
    "training_clients_pl = dict()\n",
    "\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad0cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client in CLIENTS:\n",
    "    training_clients[client] = DataLoader(breast_dataset[client+'_train'], batch_size=16,\\\n",
    "                 shuffle=True, num_workers=8)\n",
    "    training_clients_pl[client] = DataLoader(breast_dataset[client+'_train'], batch_size=1, \\\n",
    "                shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(breast_dataset[client+'_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52fbb5",
   "metadata": {},
   "source": [
    "## FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e96d5781",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py\", line 165, in __getitem__\n    w_min, w_max, h_min, h_max = torch.min(w)-margin[0], \\\nRuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#### conduct training #####\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client, supervision_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(CLIENTS, CLIENTS_SUPERVISION):\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_clients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                              \u001b[49m\u001b[43macc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43macc_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msupervision_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msupervision_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m aggr_fed(CLIENTS, WEIGHTS_CL, nets, fed_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m################### test ##############################\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py:448\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(trainloader, net_stu, optimizer_stu, device, acc, loss, supervision_type, warmup, CE_LOSS, FedMix_network)\u001b[0m\n\u001b[0;32m    446\u001b[0m labeled_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(trainloader)\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(labeled_len):\n\u001b[1;32m--> 448\u001b[0m     imgs, masks, y_pl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabeled_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m     imgs, masks \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    450\u001b[0m     optimizer_stu\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py\", line 165, in __getitem__\n    w_min, w_max, h_min, h_max = torch.min(w)-margin[0], \\\nRuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n"
     ]
    }
   ],
   "source": [
    "best_avg_acc, best_epoch_avg = 0, 0\n",
    "index = []\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    index.append(epoch)\n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    \n",
    "    #### conduct training #####\n",
    "    for client, supervision_t in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "        train_model(training_clients[client], nets[client], \\\n",
    "                                  optimizers[client], device, \\\n",
    "                                  acc = acc_train[client], \\\n",
    "                                  loss = loss_train[client], \\\n",
    "                                  supervision_type = supervision_t)\n",
    "        \n",
    "    aggr_fed(CLIENTS, WEIGHTS_CL, nets, fed_name='global')\n",
    "    ################### test ##############################\n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test(epoch, testing_clients[client], nets['global'], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        \n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    plt.figure(0)\n",
    "    plt.plot(index, acc_train['miccai'], colors[1], label='miccai train')\n",
    "    plt.plot(index, acc_train['bns'], colors[2], label='bns train')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(index, loss_train['miccai'], colors[1], label='miccai loss train')\n",
    "    plt.plot(index, loss_train['bns'], colors[2], label='bns loss train')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82951d55-7ea8-4075-b7c4-7ce85a6c7bdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py\", line 165, in __getitem__\n    w_min, w_max, h_min, h_max = torch.min(w)-margin[0], \\\nRuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Verifica il caricamento delle immagini e delle etichette nel dataloader di allenamento\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels, bboxes \u001b[38;5;129;01min\u001b[39;00m training_clients[CLIENTS[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Stampa le dimensioni delle immagini e delle etichette\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensioni delle immagini nel batch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensioni delle etichette (maschere) nel batch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py\", line 165, in __getitem__\n    w_min, w_max, h_min, h_max = torch.min(w)-margin[0], \\\nRuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.\n"
     ]
    }
   ],
   "source": [
    "# Verifica il caricamento delle immagini e delle etichette nel dataloader di allenamento\n",
    "for images, labels, bboxes in training_clients[CLIENTS[0]]:\n",
    "    # Stampa le dimensioni delle immagini e delle etichette\n",
    "    print(\"Dimensioni delle immagini nel batch:\", images.shape)\n",
    "    print(\"Dimensioni delle etichette (maschere) nel batch:\", labels.shape)\n",
    "    print(\"Dimensioni delle maschere della bounding box nel batch:\", bboxes.shape)\n",
    "    # Verifica il tipo di dato delle immagini e delle etichette\n",
    "    print(\"Tipo di dato delle immagini:\", images.dtype)\n",
    "    print(\"Tipo di dato delle etichette (maschere):\", labels.dtype)\n",
    "    print(\"Tipo di dato delle maschere della bounding box:\", bboxes.dtype)\n",
    "    # Verifica se le dimensioni delle immagini e delle etichette sono coerenti\n",
    "    assert images.shape[2:] == labels.shape[1:], \"Le dimensioni delle immagini e delle etichette non corrispondono\"\n",
    "    # Esci dal ciclo dopo aver verificato il primo batch\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48edc60-d765-4476-9b40-a43d0982f5a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ottieni un batch di dati dal dataset di allenamento del primo cliente\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m batch_images, batch_labels, batch_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mbreast_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCLIENTS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Visualizza il batch di dati\u001b[39;00m\n\u001b[0;32m     26\u001b[0m visualize_batch(batch_images, batch_labels, batch_bboxes)\n",
      "File \u001b[1;32m~\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py:165\u001b[0m, in \u001b[0;36mCancer.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     margin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m4\u001b[39m,))\n\u001b[1;32m--> 165\u001b[0m w_min, w_max, h_min, h_max \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39mmargin[\u001b[38;5;241m0\u001b[39m], \\\n\u001b[0;32m    166\u001b[0m torch\u001b[38;5;241m.\u001b[39mmax(w)\u001b[38;5;241m+\u001b[39mmargin[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[0;32m    167\u001b[0m torch\u001b[38;5;241m.\u001b[39mmin(h)\u001b[38;5;241m-\u001b[39mmargin[\u001b[38;5;241m2\u001b[39m], \\\n\u001b[0;32m    168\u001b[0m torch\u001b[38;5;241m.\u001b[39mmax(h)\u001b[38;5;241m+\u001b[39mmargin[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    169\u001b[0m w_min \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(w_min, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m    170\u001b[0m w_max \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(w_max, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci una funzione per visualizzare un singolo batch di dati\n",
    "def visualize_batch(images, labels, bboxes):\n",
    "    for i in range(len(images)):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(images[i].squeeze(), cmap='gray')\n",
    "        axes[0].set_title(f\"Image {i+1}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(labels[i].squeeze(), cmap='gray')\n",
    "        axes[1].set_title(f\"Label {i+1}\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(bboxes[i].squeeze(), cmap='gray')\n",
    "        axes[2].set_title(f\"BBox {i+1}\")\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Ottieni un batch di dati dal dataset di allenamento del primo cliente\n",
    "batch_images, batch_labels, batch_bboxes = breast_dataset[CLIENTS[0]+'_train'][0]\n",
    "\n",
    "# Visualizza il batch di dati\n",
    "visualize_batch(batch_images, batch_labels, batch_bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279452d3-c6fb-4e9a-931a-7c5b3a691360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
