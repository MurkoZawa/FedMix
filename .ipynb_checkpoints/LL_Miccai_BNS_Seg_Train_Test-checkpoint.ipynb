{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8355f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "####################################################\n",
    "# for data splitting\n",
    "####################################################\n",
    "import pandas as pd\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "###################################\n",
    "TRAIN_RATIO = 0.8\n",
    "RS = 30448\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 4, 500\n",
    "IMAGE_SIZE = (600, 600)\n",
    "CROP_SIZE = (600, 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c6a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a0e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n",
      "bns\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CLIENTS = ['miccai', 'bns']\n",
    "CLIENTS_2 = [cl + '_2' for cl in CLIENTS]\n",
    "###################################################################\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "DIR_DATA = 'data/imagesTrAug/'\n",
    "DIR_GT = 'data/labelsTrBW/'\n",
    "\n",
    "# Costruire una lista dei nomi dei file per ciascun nuovo dataset.\n",
    "skin_dataset = dict()\n",
    "skin_dataset['miccai'] = ['miccai_{:03d}'.format(i) for i in range(201, 227)]  # Nomi dei file per il dataset \"miccai\"\n",
    "skin_dataset['bns'] = ['bns_{:03d}'.format(i) for i in range(181, 204)]  # Nomi dei file per il dataset \"bns\"\n",
    "\n",
    "split_dataset = dict()\n",
    "STATIC_WEIGHT = [0, 0]\n",
    "order = 0\n",
    "\n",
    "for client in skin_dataset:\n",
    "    tmp = skin_dataset[client]\n",
    "    x_ = [os.path.join(DIR_DATA, f + '.png') for f in tmp]\n",
    "    y_ = [os.path.join(DIR_GT, f + '.png') for f in tmp]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_, y_, test_size=1 - TRAIN_RATIO, random_state=RS)\n",
    "\n",
    "    split_dataset[client + '_train'] = Cancer(x_train, y_train, train=True, \\\n",
    "                                              IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                              , CROP_SIZE=CROP_SIZE)\n",
    "    STATIC_WEIGHT[order] = len(x_train)\n",
    "    order += 1\n",
    "\n",
    "    split_dataset[client + '_test'] = Cancer(x_test, y_test, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "    print(client)\n",
    "\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Miccai al set di addestramento di Miccai\n",
    "miccai_remaining = ['miccai_{:03d}'.format(i) for i in range(1, 201)]  # Immagini non ancora aggiunte\n",
    "miccai_train = split_dataset['miccai_train'].data  # Percorsi delle immagini già nel set di addestramento di Miccai\n",
    "miccai_remaining = [os.path.join(DIR_DATA, f + '.png') for f in miccai_remaining if f not in miccai_train]\n",
    "split_dataset['miccai_train'].data.extend(miccai_remaining)\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Bns al set di addestramento di Bns\n",
    "bns_remaining = ['bns_{:03d}'.format(i) for i in range(1, 181)]  # Immagini non ancora aggiunte\n",
    "bns_train = split_dataset['bns_train'].data  # Percorsi delle immagini già nel set di addestramento di Bns\n",
    "bns_remaining = [os.path.join(DIR_DATA, f + '.png') for f in bns_remaining if f not in bns_train]\n",
    "split_dataset['bns_train'].data.extend(bns_remaining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32bf1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5263157894736842, 0.47368421052631576]\n"
     ]
    }
   ],
   "source": [
    "STATIC_WEIGHT = [item / sum(STATIC_WEIGHT) for item in STATIC_WEIGHT]\n",
    "print(STATIC_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8472ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "LR, WD, TH = 1e-3, 1e-4, 0.9\n",
    "best_avg_acc, best_epoch = 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fc565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n",
      "bns\n"
     ]
    }
   ],
   "source": [
    "training_clients, testing_clients = dict(), dict()\n",
    "training_clients_pl = dict()\n",
    "\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=4,\\\n",
    "                 shuffle=True, num_workers=8)\n",
    "    training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8cca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS_SUPERVISION = ['labeled', 'labeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b64c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc, best_epoch_avg = 0,0\n",
    "index = []\n",
    "best_model_dir = \"./\"\n",
    "best_avg_acc, best_epoch_avg = 0, 0\n",
    "best_models = {}  # Dizionario per memorizzare i migliori modelli per ciascun cliente\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], [] \n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    index.append(epoch)\n",
    "    ####### conduct training #####\n",
    "    for client, supervision_t in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "        print(client)\n",
    "        train_model(training_clients[client], nets[client], \\\n",
    "                                  optimizers[client], device, \\\n",
    "                                  acc = acc_train[client], \\\n",
    "                                  loss = loss_train[client], \\\n",
    "                                  supervision_type = supervision_t)\n",
    "       \n",
    "    ################### test ##############################\n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test(epoch, testing_clients[client], nets[client], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "         # Salva il modello se l'accuratezza del cliente migliora\n",
    "        if acc_test[client][-1] > best_models.get(client, {'accuracy': 0})['accuracy']:\n",
    "            best_models[client] = {'model': nets[client].state_dict(), 'accuracy': acc_test[client][-1]}\n",
    "            torch.save(nets[client].state_dict(), os.path.join(best_model_dir, f\"best_model_{client}.pt\"))\n",
    "\n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "        # Salvataggio del modello federato contenente tutte le reti\n",
    "        torch.save(nets['global'].state_dict(), os.path.join(best_model_dir, f\"best_fed_model.pt\"))\n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    plot_graphs(0, CLIENTS, index, acc_train, 'acc_train')\n",
    "    plot_graphs(1, CLIENTS, index, loss_train, 'loss_train')\n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedd353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
