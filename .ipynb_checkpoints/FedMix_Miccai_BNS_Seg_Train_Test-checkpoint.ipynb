{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7dbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "####################################################\n",
    "# for data splitting\n",
    "####################################################\n",
    "import pandas as pd\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "###################################\n",
    "TRAIN_RATIO = 0.8\n",
    "RS = 30448\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 4, 500\n",
    "IMAGE_SIZE = (600, 600)\n",
    "CROP_SIZE = (600, 600)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886ff19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30674cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n",
      "bns\n"
     ]
    }
   ],
   "source": [
    "CLIENTS = ['miccai', 'bns']\n",
    "CLIENTS_2 = [cl + '_2' for cl in CLIENTS]\n",
    "###################################################################\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "DIR_DATA = 'data/imagesTrAug/'\n",
    "DIR_GT = 'data/labelsTrBW/'\n",
    "\n",
    "skin_dataset = dict()\n",
    "skin_dataset['miccai'] = ['miccai_{:03d}'.format(i) for i in range(201, 227)]\n",
    "skin_dataset['bns'] = ['bns_{:03d}'.format(i) for i in range(181, 204)]\n",
    "\n",
    "split_dataset = dict()\n",
    "STATIC_WEIGHT = [0,0]\n",
    "order = 0\n",
    "\n",
    "for client in skin_dataset:\n",
    "    tmp = skin_dataset[client]\n",
    "    x_ = [os.path.join(DIR_DATA, f + '.png') for f in tmp]\n",
    "    y_ = [os.path.join(DIR_GT, f + '.png') for f in tmp]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_, y_, test_size=1 - TRAIN_RATIO, random_state=RS)\n",
    "\n",
    "    split_dataset[client + '_train'] = Cancer(x_train, y_train, train=True, \\\n",
    "                                              IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                              , CROP_SIZE=CROP_SIZE)\n",
    "    STATIC_WEIGHT[order] = len(x_train)\n",
    "    order += 1\n",
    "\n",
    "    split_dataset[client + '_test'] = Cancer(x_test, y_test, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "    print(client)\n",
    "\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Miccai al set di addestramento di Miccai\n",
    "miccai_remaining = ['miccai_{:03d}'.format(i) for i in range(1, 201)]  # Immagini non ancora aggiunte\n",
    "miccai_train = split_dataset['miccai_train'].data  # Percorsi delle immagini già nel set di addestramento di Miccai\n",
    "miccai_remaining = [os.path.join(DIR_DATA, f + '.png') for f in miccai_remaining if f not in miccai_train]\n",
    "split_dataset['miccai_train'].data.extend(miccai_remaining)\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Bns al set di addestramento di Bns\n",
    "bns_remaining = ['bns_{:03d}'.format(i) for i in range(1, 181)]  # Immagini non ancora aggiunte\n",
    "bns_train = split_dataset['bns_train'].data  # Percorsi delle immagini già nel set di addestramento di Bns\n",
    "bns_remaining = [os.path.join(DIR_DATA, f + '.png') for f in bns_remaining if f not in bns_train]\n",
    "split_dataset['bns_train'].data.extend(bns_remaining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc254ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5263157894736842, 0.47368421052631576]\n"
     ]
    }
   ],
   "source": [
    "STATIC_WEIGHT = [item / sum(STATIC_WEIGHT) for item in STATIC_WEIGHT]\n",
    "print(STATIC_WEIGHT)\n",
    "WEIGHTS = STATIC_WEIGHT\n",
    "WEIGHTS_DATA = copy.deepcopy(WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdd5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "LR, WD, TH = 1e-3, 1e-4, 0.9\n",
    "best_avg_acc, best_epoch = 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3223805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n",
      "bns\n"
     ]
    }
   ],
   "source": [
    "training_clients, testing_clients = dict(), dict()\n",
    "training_clients_pl = dict()\n",
    "\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=4,\\\n",
    "                 shuffle=True, num_workers=8)\n",
    "    training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48e6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIENTS_SUPERVISION = ['unlabeled', 'unlabeled', 'labeled', 'unlabeled'] # [U,U,L,U]\n",
    "CLIENTS_SUPERVISION = ['labeled','labeled'] # [B, B, L, B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a26712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_ = 10\n",
    "BETA_ = 3\n",
    "TH = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524c195",
   "metadata": {},
   "source": [
    "# FedMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_avg_acc, best_epoch_avg = 0,0\n",
    "index = []\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "\n",
    "WEIGHTS = [0.0, 0.0]\n",
    "DATA_NUM = [0,0]\n",
    "score = [0,0]\n",
    "count=0\n",
    "\n",
    "for epoch in range(EPOCHS):        \n",
    "    index.append(epoch)\n",
    "    DATA_NUM = [0,0]\n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    copy_fed(CLIENTS_2, nets, fed_name='global_2')\n",
    "    \n",
    "    ######################################################\n",
    "    # generate and refine pseudo labels ##################\n",
    "    ######################################################\n",
    "    for order, client in enumerate(CLIENTS):\n",
    "        bbox, image = False, False\n",
    "        if CLIENTS_SUPERVISION[order] == 'labeled':\n",
    "            count=0\n",
    "        elif CLIENTS_SUPERVISION[order] == 'bbox':\n",
    "            bbox = True\n",
    "        elif CLIENTS_SUPERVISION[order] == 'image':\n",
    "            image= True\n",
    "        ##################################################\n",
    "        # save pl ########################################\n",
    "        ##################################################\n",
    "        im_store, pl1_store, pl2_store = [], [], []\n",
    "        \n",
    "        tmp_ = select_pl_singleclass(nets['global'], nets['global_2'], device,\\\n",
    "                      training_clients_pl[client], im_store, pl1_store, \\\n",
    "                      pl2_store, TH=TH, bbox=bbox, image=image)\n",
    "        \n",
    "        if len(im_store) >= 1:\n",
    "            tmp_dataset = cancer_v2(im_store, pl1_store, pl2_store)\n",
    "            training_clients[client] = DataLoader(tmp_dataset, batch_size=4,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "\n",
    "        DATA_NUM[order] = tmp_\n",
    "    #######################################################\n",
    "    #### Conduct training #################################\n",
    "    #######################################################\n",
    "    for order, (client, supervision_t) in enumerate(zip(CLIENTS, CLIENTS_SUPERVISION)):\n",
    "        if supervision_t == 'labeled':\n",
    "            # train network 1 #\n",
    "            train_model(training_clients[client], nets[client], optimizers[client], device, \\\n",
    "                       acc=acc_train[client], loss=loss_train[client], \\\n",
    "                        supervision_type=supervision_t)\n",
    "            \n",
    "            # train network 2 # \n",
    "            train_model(training_clients[client], nets[client+'_2'], optimizers[client+'_2'], device, \\\n",
    "                       acc=None, loss=None, \\\n",
    "                        supervision_type=supervision_t)\n",
    "            \n",
    "        else: # train using pseudo label # \n",
    "            # train network 1 #\n",
    "            train_model(training_clients[client], nets[client], optimizers[client], device, \\\n",
    "                       acc=acc_train[client], loss=loss_train[client], \\\n",
    "                        supervision_type=supervision_t, FedMix_network=1)\n",
    "            \n",
    "            # train network 2 # \n",
    "            train_model(training_clients[client], nets[client+'_2'], optimizers[client+'_2'], device, \\\n",
    "                       acc=None, loss=None, \\\n",
    "                        supervision_type=supervision_t, FedMix_network=2)\n",
    "        \n",
    "        \n",
    "        # save loss for future reweighting # \n",
    "        score[order] = loss_train[client][-1] ** BETA_\n",
    "    ###################################\n",
    "    ####### dynamic weighting #########\n",
    "    ###################################\n",
    "    denominator = sum(score)\n",
    "    score = [s/denominator for s in score]\n",
    "    \n",
    "    denominator = sum(DATA_NUM)\n",
    "    WEIGHTS_CL = [s/denominator for s in DATA_NUM]\n",
    "    for order, _ in enumerate(WEIGHTS):\n",
    "        WEIGHTS[order] = WEIGHTS_CL[order] + LAMBDA_ * score[order]\n",
    "        \n",
    "    ### normalize #####################\n",
    "    denominator = sum(WEIGHTS)\n",
    "    WEIGHTS = [w/denominator for w in WEIGHTS]\n",
    "\n",
    "    ###################################\n",
    "    ####### aggregation ###############\n",
    "    ###################################\n",
    "    aggr_fed(CLIENTS, WEIGHTS, nets, fed_name='global')\n",
    "    aggr_fed(CLIENTS_2, WEIGHTS, nets, fed_name='global_2')\n",
    "    \n",
    "    \n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test(epoch, testing_clients[client], nets['global'], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        \n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "        # Salvataggio del modello per Miccai e Bns\n",
    "        torch.save(nets['miccai'].state_dict(), 'best_model_miccai.pt')\n",
    "        torch.save(nets['bns'].state_dict(), 'best_model_bns.pt')\n",
    "        torch.save(nets['global'].state_dict(), 'best_fed_model.pt')\n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    print(WEIGHTS)\n",
    "    plot_graphs(0, CLIENTS, index, acc_train, 'acc_train')\n",
    "    plot_graphs(1, CLIENTS, index, loss_train, 'loss_train')\n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd562763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
