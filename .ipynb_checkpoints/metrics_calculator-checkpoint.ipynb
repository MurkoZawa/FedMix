{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ff59bf-9bfb-4e6d-bc28-5e525f8a2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n",
      "bns\n",
      "monuseg\n",
      "miccai\n",
      "bns\n",
      "monuseg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper import * \n",
    "from unet import UNet\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "RS = 30448\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 16, 500\n",
    "IMAGE_SIZE = (600, 600)\n",
    "CROP_SIZE = (600, 600)\n",
    "device = torch.device('cuda:0')\n",
    "LR, WD, TH = 1e-3, 1e-4, 0.9\n",
    "best_avg_acc, best_epoch = 0.0, 0\n",
    "CLIENTS = ['miccai', 'bns', 'monuseg']\n",
    "CLIENTS_2 = [cl + '_2' for cl in CLIENTS]\n",
    "###################################################################\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "DIR_DATA = 'data/imagesTrAug/'\n",
    "DIR_GT = 'data/labelsTrBW/'\n",
    "\n",
    "# Costruire una lista dei nomi dei file per ciascun nuovo dataset.\n",
    "skin_dataset = dict()\n",
    "skin_dataset['miccai'] = ['miccai_{:03d}'.format(i) for i in range(201, 227)]  # Nomi dei file per il dataset \"miccai\"\n",
    "skin_dataset['bns'] = ['bns_{:03d}'.format(i) for i in range(181, 204)]  # Nomi dei file per il dataset \"bns\"\n",
    "skin_dataset['monuseg'] = ['monuseg_{:03d}'.format(i) for i in range(1, 52)]  # Nomi dei file per il dataset \"monuseg\"\n",
    "split_dataset = dict()\n",
    "STATIC_WEIGHT = [0, 0, 0]\n",
    "order = 0\n",
    "\n",
    "for client in skin_dataset:\n",
    "    tmp = skin_dataset[client]\n",
    "    x_ = [os.path.join(DIR_DATA, f + '.png') for f in tmp]\n",
    "    y_ = [os.path.join(DIR_GT, f + '.png') for f in tmp]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_, y_, test_size=1 - TRAIN_RATIO, random_state=RS)\n",
    "\n",
    "    split_dataset[client + '_train'] = Cancer(x_train, y_train, train=True, \\\n",
    "                                              IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                              , CROP_SIZE=CROP_SIZE)\n",
    "    STATIC_WEIGHT[order] = len(x_train)\n",
    "    order += 1\n",
    "\n",
    "    split_dataset[client + '_test'] = Cancer(x_test, y_test, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "    print(client)\n",
    "\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Miccai al set di addestramento di Miccai\n",
    "miccai_remaining = ['miccai_{:03d}'.format(i) for i in range(1, 201)]  # Immagini non ancora aggiunte\n",
    "miccai_train = split_dataset['miccai_train'].data  # Percorsi delle immagini già nel set di addestramento di Miccai\n",
    "miccai_remaining = [os.path.join(DIR_DATA, f + '.png') for f in miccai_remaining if f not in miccai_train]\n",
    "split_dataset['miccai_train'].data.extend(miccai_remaining)\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Bns al set di addestramento di Bns\n",
    "bns_remaining = ['bns_{:03d}'.format(i) for i in range(1, 181)]  # Immagini non ancora aggiunte\n",
    "bns_train = split_dataset['bns_train'].data  # Percorsi delle immagini già nel set di addestramento di Bns\n",
    "bns_remaining = [os.path.join(DIR_DATA, f + '.png') for f in bns_remaining if f not in bns_train]\n",
    "split_dataset['bns_train'].data.extend(bns_remaining)\n",
    "\n",
    "# Aggiungi le immagini rimanenti di Bns al set di addestramento di monuseg\n",
    "monuseg_remaining = ['monuseg_{:03d}'.format(i) for i in range(52, 155)]  # Immagini non ancora aggiunte\n",
    "monuseg_train = split_dataset['monuseg_train'].data  # Percorsi delle immagini già nel set di addestramento di monuseg\n",
    "monuseg_remaining = [os.path.join(DIR_DATA, f + '.png') for f in monuseg_remaining if f not in monuseg_train]\n",
    "split_dataset['monuseg_train'].data.extend(monuseg_remaining)\n",
    "\n",
    "training_clients, testing_clients = dict(), dict()\n",
    "training_clients_pl = dict()\n",
    "\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=4,\\\n",
    "                 shuffle=True, num_workers=8)\n",
    "    training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eec3f7-9271-4f8a-92db-7fcfa5799d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa per testare col dataset monuseg\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper import * \n",
    "from unet import UNet\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "RS = 30448\n",
    "N_CHANNELS, N_CLASSES = 1, 1 \n",
    "bilinear = True\n",
    "BATCH_SIZE, EPOCHS = 16, 500\n",
    "IMAGE_SIZE = (1000, 1000)\n",
    "CROP_SIZE = (1000, 1000)\n",
    "device = torch.device('cuda:0')\n",
    "LR, WD, TH = 1e-3, 1e-4, 0.9\n",
    "best_avg_acc, best_epoch = 0.0, 0\n",
    "CLIENTS = ['monuseg', 'tnbc']\n",
    "CLIENTS_2 = [cl + '_2' for cl in CLIENTS]\n",
    "###################################################################\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "training_clients, testing_clients = dict(), dict()\n",
    "training_clients_pl = dict()\n",
    "\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()\n",
    "DIR_DATA = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\MoNuSegTestData'\n",
    "DIR_GT = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\MoNuSegLabelPNGTestData'\n",
    "DIR_DATA2 = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\imagesTNBC'\n",
    "DIR_GT2 = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\LabelsTNBC'\n",
    "DIR_DATA3 = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\imagesMoNuSAC'\n",
    "DIR_GT3 = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\labelsMoNuSAC'\n",
    "# Costruire una lista dei nomi dei file per ciascun nuovo dataset.\n",
    "skin_dataset = dict()\n",
    "skin_dataset['monuseg'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/MoNuSegTestData\") if f.startswith(\"monu\")]  \n",
    "skin_dataset['monuseglbl'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/MoNuSegLabelPNGTestData\") if f.startswith(\"TCGA\")]  \n",
    "split_dataset = dict()\n",
    "STATIC_WEIGHT = [0, 0]\n",
    "order = 0\n",
    "\n",
    "x_ = [os.path.join(DIR_DATA, f) for f in skin_dataset['monuseg']]\n",
    "y_ = [os.path.join(DIR_GT, f) for f in skin_dataset['monuseglbl']]\n",
    "\n",
    "STATIC_WEIGHT[order] = len(x_)\n",
    "order += 1\n",
    "\n",
    "split_dataset['monuseg_test'] = Cancer(x_, y_, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "\n",
    "testing_clients['monuseg'] = DataLoader(split_dataset['monuseg_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "\n",
    "skin_dataset['tnbc'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/imagesTNBC\") if f.endswith(\"png\")]  \n",
    "skin_dataset['tnbclbl'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/LabelsTNBC\") if f.endswith(\"png\")]  \n",
    "order = 0\n",
    "\n",
    "x_ = [os.path.join(DIR_DATA2, f) for f in skin_dataset['tnbc']]\n",
    "y_ = [os.path.join(DIR_GT2, f) for f in skin_dataset['tnbc']]\n",
    "\n",
    "STATIC_WEIGHT[order] = len(x_)\n",
    "order += 1\n",
    "\n",
    "split_dataset['tnbc_test'] = Cancer(x_, y_, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "\n",
    "testing_clients['tnbc'] = DataLoader(split_dataset['tnbc_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "\n",
    "skin_dataset['monusac'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/imagesMoNuSAC\") if f.endswith(\"png\")]  \n",
    "skin_dataset['monusaclbl'] = [f for f in os.listdir(\"C:/Users/utente/Desktop/Università/Tesi magistrale/FedMix/data/labelsMoNuSAC\") if f.endswith(\"png\")]  \n",
    "order = 0\n",
    "\n",
    "x_ = [os.path.join(DIR_DATA3, f) for f in skin_dataset['monusac']]\n",
    "y_ = [os.path.join(DIR_GT3, f) for f in skin_dataset['monusac']]\n",
    "\n",
    "STATIC_WEIGHT[order] = len(x_)\n",
    "order += 1\n",
    "\n",
    "split_dataset['monusac_test'] = Cancer(x_, y_, train=False, \\\n",
    "                                             IMAGE_SIZE=IMAGE_SIZE \\\n",
    "                                             , CROP_SIZE=CROP_SIZE)\n",
    "\n",
    "testing_clients['monusac'] = DataLoader(split_dataset['monusac_test'], batch_size=1,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "    \n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3845ada9-6d47-4782-a44f-36279da61c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_score_(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    total_pixel_pred = np.sum(pred_mask)\n",
    "    precision = np.mean(intersect/total_pixel_pred)\n",
    "    return round(precision, 3)\n",
    "\n",
    "def recall_score_(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    total_pixel_truth = np.sum(groundtruth_mask)\n",
    "    recall = np.mean(intersect/total_pixel_truth)\n",
    "    return round(recall, 3)\n",
    "\n",
    "def accuracy(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
    "    xor = np.sum(groundtruth_mask==pred_mask)\n",
    "    acc = np.mean(xor/(union + xor - intersect))\n",
    "    return round(acc, 3)\n",
    "\n",
    "def dice_coef(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    total_sum = np.sum(pred_mask) + np.sum(groundtruth_mask)\n",
    "    dice = np.mean(2*intersect/total_sum)\n",
    "    return round(dice, 3) #round up to 3 decimal places\n",
    "\n",
    "def iou(groundtruth_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*groundtruth_mask)\n",
    "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
    "    iou = np.mean(intersect/union)\n",
    "    return round(iou, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fda78cd2-938b-4776-a3c9-a1e83d90f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.648\n",
      "Precision: 0.302\n",
      "Recall: 1.0\n",
      "Accuracy: 0.302\n",
      "Dice Coef: 0.441\n",
      "Jaccard Index: 0.302\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def test_ultimate(epoch, testloader, net, device, acc=None, loss=None):\n",
    "    net.eval()\n",
    "    t_loss, t_acc = 0, 0\n",
    "    t_precision, t_recall, t_accuracy, t_dice, t_iou = 0, 0, 0, 0, 0\n",
    "    num_samples = len(testloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (imgs, masks, _) in testloader:\n",
    "            masks = masks.type(torch.float32)\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "            masks_pred = net(imgs)\n",
    "            masks_pred = torch.sigmoid(masks_pred)\n",
    "            l_ = 1 - dice_coeff(masks_pred, masks.type(torch.float))\n",
    "            t_loss += l_.item()\n",
    "            \n",
    "            masks_pred = (masks_pred > 0.5).float()\n",
    "            \n",
    "           \n",
    "            for i in range(imgs.size(0)):\n",
    "                pred_mask = masks_pred[i].squeeze().cpu().numpy()\n",
    "                gt_mask = masks[i].squeeze().cpu().numpy()\n",
    "                t_precision += precision_score_(gt_mask, pred_mask)\n",
    "                t_recall += recall_score_(gt_mask, pred_mask)\n",
    "                t_accuracy += accuracy(gt_mask, pred_mask)\n",
    "                t_dice += dice_coef(gt_mask, pred_mask)\n",
    "                t_iou += iou(gt_mask, pred_mask)\n",
    "            \n",
    "            #plt.figure(figsize=(12, 6))\n",
    "            #plt.subplot(1, 2, 1)\n",
    "            #plt.imshow(imgs.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "            #plt.title('Original Image')\n",
    "            #plt.axis('off')\n",
    "\n",
    "            #plt.subplot(1, 2, 2)\n",
    "            #plt.imshow(masks_pred.squeeze(0).squeeze(0).cpu().numpy(), cmap='gray')\n",
    "            #plt.title('Segmentation Prediction')\n",
    "            #plt.axis('off')\n",
    "            #plt.show()\n",
    "\n",
    " \n",
    "    t_loss /= len(testloader)\n",
    "    t_precision /= num_samples\n",
    "    t_recall /= num_samples\n",
    "    t_accuracy /= num_samples\n",
    "    t_dice /= num_samples\n",
    "    t_iou /= num_samples\n",
    "    \n",
    "\n",
    "    print(f\"Test Loss: {round(t_loss, 3)}\")\n",
    "    print(f\"Precision: {round(t_precision, 3)}\")\n",
    "    print(f\"Recall: {round(t_recall, 3)}\")\n",
    "    print(f\"Accuracy: {round(t_accuracy, 3)}\")\n",
    "    print(f\"Dice Coef: {round(t_dice, 3)}\")\n",
    "    print(f\"Jaccard Index: {round(t_iou, 3)}\")\n",
    "    \n",
    "    if acc is not None:\n",
    "        acc.append(t_accuracy)\n",
    "    if loss is not None:\n",
    "        loss.append(t_loss)\n",
    "   \n",
    "    del t_acc, t_loss\n",
    "\n",
    "model_path = r\"C:\\Users\\utente\\Desktop\\best_model_bns.pt\"\n",
    "testing_loader = testing_clients['monusac']\n",
    "net = nets['global']\n",
    "\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_ultimate(323, testing_loader, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442cdb3-6603-450c-a3bb-613e8081955b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
