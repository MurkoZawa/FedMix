{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f014caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from unet import UNet\n",
    "from dice_loss import dice_coeff\n",
    "####################################################\n",
    "# for data splitting\n",
    "####################################################\n",
    "import pandas as pd\n",
    "####################################################\n",
    "# for data preparation\n",
    "####################################################\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "####################################################\n",
    "# for plotting\n",
    "####################################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "############################\n",
    "# Helper func\n",
    "############################\n",
    "from helper import * \n",
    "\n",
    "########################################\n",
    "N_CHANNELS = 1 #greyscale\n",
    "N_CLASSES = 2 # classes, IRF, SRF, PED\n",
    "\n",
    "BATCH_SIZE, EPOCHS = 16, 150\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CROP_SIZE = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4c626c-ff9f-4e18-a942-2cd05157a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748743718592965\n",
      "0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "PTH = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\labelsTrBW'\n",
    "PTH_IM = r'C:\\Users\\utente\\Desktop\\Università\\Tesi magistrale\\FedMix\\data\\imagesTrBW'\n",
    "\n",
    "miccai_ = np.arange(1,200)\n",
    "bns_ = np.arange(200,380)\n",
    "\n",
    "miccai_test = np.random.choice(miccai_, size=5, replace=False)\n",
    "miccai_test = [str(a) for a in miccai_test]\n",
    "miccai_train = [str(a) for a in miccai_ if str(a) not in miccai_test]\n",
    "print(len(miccai_train)/len(miccai_))\n",
    "\n",
    "bns_test = np.random.choice(bns_, size=5, replace=False)\n",
    "bns_test = [str(a) for a in bns_test]\n",
    "bns_train = [str(a) for a in bns_ if str(a) not in bns_test]\n",
    "print(len(bns_train)/len(bns_))\n",
    "\n",
    "whole_data =  os.listdir(PTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feefc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = dict()\n",
    "TEST_SPLIT['miccai'] = miccai_test\n",
    "TEST_SPLIT['bns'] = bns_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a34d0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHOLE_DATA_TRAIN = dict()\n",
    "WHOLE_DATA_TEST = dict()\n",
    "\n",
    "WHOLE_DATA_TRAIN['miccai'] = []\n",
    "WHOLE_DATA_TRAIN['bns'] = []\n",
    "\n",
    "WHOLE_DATA_TEST['miccai'] = []\n",
    "WHOLE_DATA_TEST['bns'] = []\n",
    "\n",
    "import random\n",
    "\n",
    "# Calcola il numero totale di campioni\n",
    "total_samples = len(whole_data)\n",
    "\n",
    "# Calcola il numero di campioni da utilizzare per il test\n",
    "test_size = int(0.1 * total_samples)\n",
    "\n",
    "# Estrai casualmente un subset di campioni per il test\n",
    "test_samples = random.sample(whole_data, test_size)\n",
    "\n",
    "# Itera attraverso i dati e assegna le immagini e le label al set di test se sono presenti nei campioni selezionati\n",
    "for item in whole_data:\n",
    "    separator = item.split('_') # Identifica la fonte\n",
    "    sample_number = separator[1].split('0')[-1]\n",
    "    \n",
    "    # Controlla se l'elemento corrente è presente nei campioni di test\n",
    "    if item in test_samples:\n",
    "        # Aggiungi l'elemento al set di test\n",
    "        data_path = (PTH_IM+'\\\\'+item, PTH+'\\\\'+item) # X,Y source\n",
    "        WHOLE_DATA_TEST[separator[0]].append(data_path)\n",
    "    else:\n",
    "        # Aggiungi l'elemento al set di allenamento\n",
    "        data_path = (PTH_IM+'\\\\'+item, PTH+'\\\\'+item) # X,Y source\n",
    "        WHOLE_DATA_TRAIN[separator[0]].append(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab83d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\t\t train\t test\n",
      "miccai \t\t 175 \t 25\n",
      "bns \t 329 \t 31\n"
     ]
    }
   ],
   "source": [
    "LEN_bns = 0\n",
    "print('name\\t\\t train\\t test')\n",
    "for item in WHOLE_DATA_TRAIN:\n",
    "    if item == 'bns':\n",
    "        print(item,'\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))\n",
    "        LEN_bns = WHOLE_DATA_TRAIN[item]\n",
    "    else:\n",
    "        print(item,'\\t\\t', len(WHOLE_DATA_TRAIN[item]),'\\t', len(WHOLE_DATA_TEST[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67948c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "WEIGHTS_CL = [0.0,0.0]\n",
    "CLIENTS = ['miccai', 'bns']\n",
    "CLIENTS_2 = [cl +'_2' for cl in CLIENTS]\n",
    "TOTAL_CLIENTS = len(CLIENTS)\n",
    "\n",
    "LR = 1.5e-3\n",
    "WD = 1e-5\n",
    "TH = 0.9\n",
    "\n",
    "LAMBDA_ =2\n",
    "BETA_=3\n",
    "TH = 0.9\n",
    "\n",
    "for idx, client in enumerate(WHOLE_DATA_TRAIN):\n",
    "    WEIGHTS_CL[idx] = len(WHOLE_DATA_TRAIN[client])\n",
    "\n",
    "    \n",
    "total_weight = sum(WEIGHTS_CL)\n",
    "WEIGHTS_CL = [s/total_weight for s in WEIGHTS_CL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8144b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS_SUPERVISION = ['labeled', 'labeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d4ad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dict()\n",
    "for cl in CLIENTS:\n",
    "    split_dataset[cl+'_train'] = retouch(WHOLE_DATA_TRAIN[cl], train=True)\n",
    "    split_dataset[cl+'_test'] = retouch(WHOLE_DATA_TEST[cl], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777da8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CLIENTS = 2\n",
    "GLOBAL_ACC = 0.0\n",
    "\n",
    "training_clients, testing_clients = dict(), dict()\n",
    "########## aditional #####################\n",
    "training_clients_pl = dict()\n",
    "acc_train, acc_test, loss_train, loss_test = dict(), dict(), \\\n",
    "                                            dict(), dict()\n",
    "\n",
    "nets, optimizers = dict(), dict()\n",
    "\n",
    "nets['global'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "nets['global_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "\n",
    "for client, c_sup in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "    if c_sup == 'labeled':\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                     shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1, \\\n",
    "                    shuffle=True, num_workers=8)\n",
    "    else:\n",
    "        training_clients[client] = DataLoader(split_dataset[client+'_train'], batch_size=16,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        ################# additional dataloader ##########################################\n",
    "        training_clients_pl[client] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "        training_clients_pl[client+'_2'] = DataLoader(split_dataset[client+'_train'], batch_size=1,\\\n",
    "                             shuffle=True, num_workers=8)\n",
    "    ###################################################################################\n",
    "    testing_clients[client] = DataLoader(split_dataset[client+'_test'], batch_size=16,\\\n",
    "                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], []\n",
    "        \n",
    "    nets[client] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    nets[client+'_2'] = UNet(n_channels=N_CHANNELS, n_classes=N_CLASSES, \\\n",
    "                      bilinear=True).to(device)\n",
    "    optimizers[client]= optim.Adam(nets[client].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)\n",
    "    optimizers[client+'_2']= optim.Adam(nets[client+'_2'].parameters(), \\\n",
    "                                   lr=LR,weight_decay=WD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7805103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98a18193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3472222222222222, 0.6527777777777778]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73e8fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miccai\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [80000] and src [120000] to have the same number of elements, but got 80000 and 120000 elements respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client, supervision_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(CLIENTS, CLIENTS_SUPERVISION):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(client)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mtrain_model_multiclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_clients\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43macc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43macc_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msupervision_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msupervision_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m aggr_fed(CLIENTS, WEIGHTS_CL, nets, fed_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m################### test ##############################\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Università\\Tesi magistrale\\FedMix\\helper.py:522\u001b[0m, in \u001b[0;36mtrain_model_multiclasses\u001b[1;34m(trainloader, net_stu, optimizer_stu, device, acc, loss, supervision_type, warmup, CE_LOSS, TH, FedMix_network, num_client)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m### if supervision type is labeled, just train as normal with dice ###\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supervision_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabeled\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 522\u001b[0m     dc \u001b[38;5;241m=\u001b[39m \u001b[43mdice_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks_stu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     l_stu \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dc)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    524\u001b[0m     l_ \u001b[38;5;241m=\u001b[39m l_stu\n",
      "File \u001b[1;32m~\\Desktop\\Università\\Tesi magistrale\\FedMix\\dice_loss.py:40\u001b[0m, in \u001b[0;36mdice_coeff\u001b[1;34m(input, target)\u001b[0m\n\u001b[0;32m     37\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28minput\u001b[39m, target)):\n\u001b[1;32m---> 40\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m+\u001b[39m \u001b[43mDiceCoeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Università\\Tesi magistrale\\FedMix\\dice_loss.py:11\u001b[0m, in \u001b[0;36mDiceCoeff.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m     10\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(target) \u001b[38;5;241m+\u001b[39m eps\n\u001b[0;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m+\u001b[39m eps) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munion\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [80000] and src [120000] to have the same number of elements, but got 80000 and 120000 elements respectively"
     ]
    }
   ],
   "source": [
    "best_avg_acc, best_epoch_avg = 0,0\n",
    "index = []\n",
    "\n",
    "for client in CLIENTS:\n",
    "    acc_train[client], acc_test[client] = [], []\n",
    "    loss_train[client], loss_test[client] = [], [] \n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    index.append(epoch)\n",
    "    ####### conduct training #####\n",
    "    #################### copy fed model ###################\n",
    "    copy_fed(CLIENTS, nets, fed_name='global')\n",
    "    \n",
    "    for client, supervision_t in zip(CLIENTS, CLIENTS_SUPERVISION):\n",
    "        print(client)\n",
    "        train_model_multiclasses(training_clients[client], nets[client], \\\n",
    "                                  optimizers[client], device, \\\n",
    "                                  acc = acc_train[client], \\\n",
    "                                  loss = loss_train[client], \\\n",
    "                                  supervision_type = supervision_t)\n",
    "        \n",
    "    aggr_fed(CLIENTS, WEIGHTS_CL, nets, fed_name='global')\n",
    "    ################### test ##############################\n",
    "    avg_acc = 0.0\n",
    "    for client in CLIENTS:\n",
    "        test_multiclasses(epoch, testing_clients[client], nets[client], device, acc_test[client],\\\n",
    "             loss_test[client])\n",
    "        avg_acc += acc_test[client][-1]\n",
    "        \n",
    "    avg_acc = avg_acc / TOTAL_CLIENTS\n",
    "    ############################################################\n",
    "    ########################################################\n",
    "    if avg_acc > best_avg_acc:\n",
    "        best_avg_acc = avg_acc\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    ################################\n",
    "    # plot #########################\n",
    "    ################################\n",
    "    clear_output(wait=True)\n",
    "    print(avg_acc, best_avg_acc)\n",
    "    plot_graphs(0, CLIENTS, index, acc_train, 'acc_train')\n",
    "    plot_graphs(1, CLIENTS, index, loss_train, 'loss_train')\n",
    "    plot_graphs(2, CLIENTS, index, acc_test, ' acc_test')\n",
    "\n",
    "print(best_avg_acc, best_epoch)\n",
    "for client in CLIENTS:\n",
    "    print(client)\n",
    "    tmp = best_epoch\n",
    "    best_epoch = best_epoch \n",
    "    print(\"shared epoch specific\")\n",
    "    print(acc_test[client][best_epoch])\n",
    "    print(\"max client-specific\")\n",
    "    print(np.max(acc_test[client]))\n",
    "    best_epoch = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6deb95-5f40-4412-925e-6bd9e86c7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa un batch completo\n",
    "for batch_idx, batch in enumerate(training_clients[client]):\n",
    "    print(\"Batch:\", batch_idx + 1)\n",
    "    for data in batch:\n",
    "        print(\"Forma del tensore:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd9309-2dcb-4b23-aeb7-0654f4ef7cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
